""" 
This script uses the langchain_openai package to evaluate the responses generated by the model. 
The responses are evaluated based on the 4 metrics mentioned in the review_prompt. 
The evaluation is done by a domain expert and the scores are then graded by a strict grader. The final scores are then appended to a csv file. 
"""
import pandas as pd
import os
import ast
import csv
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv('../.env')

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=None,
)

def extract_csv_data(file_path):

    queries = []
    base_response = []
    gen_response = []

    try:
        with open(file_path, mode='r', newline='', encoding='utf-8') as file:
            csv_reader = csv.reader(file)
            next(csv_reader)  
            for row in csv_reader:
                queries.append(row[0]) 
                gen_response.append(row[1])  
                base_response.append(row[2])  
        
        return queries, gen_response, base_response
    
    except Exception as e:
        print(f"Error : {e}")
        
#mention the file path of dataset csv

file_path = ''  
queries, gen_response, base_response = extract_csv_data(file_path)

review_prompt = '''
You are an expert evaluator in the field of mergers and acquisitions, with extensive knowledge of corporate finance, strategic management, and market analysis.  Your task is to compare two responses (Response A and Response B) which are 
detailed reports to a given query and judge them based ONLY on the following 4 metrics:

Score1: This measures how well the response directly addresses the specific query, focusing on the alignment with the query's goals. Make sure that the response does not stray into unnecessary details that would not help in answering the query, and suitably punish such responses.
Score2: Consider the clarity of the writing and the logical flow of ideas. Focus on how well the ideas are connected across the response and if the structure of the report holds up.
Score3: Evaluate how effectively the financial metrics support the reportâ€™s arguments and recommendations and comment on the incorporation of good market analysis practices.
Score4: This focuses on how well the response incorporates numerical data, including factual numbers, statistics, or quantitative analysis. Encourage more recent data and discredit vague answers without enough reasoning and numbers backing them by highly penalizing them .

Instructions:
Review induvidually each metric for both responses and give proper explaining of how you are evaluating and comparing.
Ensure that the order in which the responses are presented do not influence your decision. 
Do not allow the length of the responses to influence your evaluation
. 
Return in json format
    {
    Score1 [
        {
        responseA: review,
        responseB: review,
        }
    ]
    ...
    }
'''

grader_prompt = '''
You are a strict grader 
Evaluate the two responses based on the query, the final json review generated by the review parser, and assign a STRICT score for each response, based on the following table
1 - The response completely fails to address the metric. 
2 - The response demonstrates some basic functionality but has critical weaknesses according to metric,
3 - The response adequately addresses the metric, but there is considerable room for improvement. 
4 - The response effectively addresses the metric with good quality, but it may lack perfection.
5 - The responses exceeds expectations and demonstrates exceptional quality and insight according to metric.
and assign a score from 1 to 5 for both responseA and responseB.
Return the scores for each response as a list in the following format:[[score1_A, Score2_A, Score3_A, Score4_A],[Score1_B,Score2_B,Score3_B, Score4_B]] AND NOTHING ELSE.
'''

def evaluate_responses(idx,query, response_a, response_b):
    prompt = (
    review_prompt  + 
    "Now, review the following:"
    f"query: {query}"
    f"Response A: {response_a}"
    f"Response B: {response_b}"
    )

    review = llm.invoke(prompt).content
    with open(f"review{idx}.txt","w") as f:
        f.write(review)
    scores = llm.invoke(f"Domain expert: {review} {grader_prompt}").content
    score_list = ast.literal_eval(scores)
    
    return score_list


with open('eval_result.csv', mode='a', newline='', encoding='utf-8') as file:
    csv_writer = csv.writer(file)
    file.seek(0,2)
    if file.tell() == 0:  # Write header if empty
        csv_writer.writerow(['Query', 'Our Response', 'Base Response', 'Our Response Score', 'Base Response Score'])
    idx=0
    for query, our_res, base_res in zip(queries, gen_response, base_response):

        scores = evaluate_responses(idx,query, our_res, base_res)
        gen_score = [scores[0][0]*2, scores[0][1]*2, scores[0][2]*2, scores[0][3]*2, ]
        base_score = [scores[1][0]*2, scores[1][1]*2, scores[1][2]*2, scores[1][3]*2, ]
        csv_writer.writerow([query, our_res, base_res, gen_score, base_score])
        idx+=1

print("Evaluation results appended to eval_results.csv")